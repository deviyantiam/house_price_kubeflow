{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kubeflow Pipeline Building - House Price model training and prediction**\n",
    "* Create python function\n",
    "    1. Download data\n",
    "    2. Prepare data (split into train test df)\n",
    "    3. Preprocess training data\n",
    "    4. Train model\n",
    "    5. Predict on test data\n",
    "    6. Get metrics\n",
    "    7. Store model to GCS\n",
    "* Create components from python functions\n",
    "* Initialise kubeflow pipeline\n",
    "* Define the pipeline function and put together all the components, include disable cache\n",
    "* Create run from pipeline function using the code\n",
    "\n",
    "![pipeline](kube_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import requests\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "from dotenv import load_dotenv\n",
    "import kfp.gcp as gcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kfp\n",
      "Version: 1.8.20\n",
      "Summary: KubeFlow Pipelines SDK\n",
      "Home-page: https://github.com/kubeflow/pipelines\n",
      "Author: The Kubeflow Authors"
     ]
    }
   ],
   "source": [
    "!pip show kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticating Kubeflow to GCP\n",
    "visit [here](https://v0-6.kubeflow.org/docs/gke/authentication/) to find the instruction, create secret in your namespace with gcp service account json. Make sure when you run **kubectl get secret -n yourkubeflow_namespace**, it returns the secret you just set. **kubectl create secret generic user-gcp-sa -n yourkubeflow_namespace --from-file=user-gcp-sa.json=path\\sa.json**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create python function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Author-email: \n",
      "License: \n",
      "Location: c:\\users\\deviy\\anaconda3\\envs\\kube_pro\\lib\\site-packages\n",
      "Requires: absl-py, click, cloudpickle, Deprecated, docstring-parser, fire, google-api-core, google-api-python-client, google-auth, google-cloud-storage, jsonschema, kfp-pipeline-spec, kfp-server-api, kubernetes, protobuf, pydantic, PyYAML, requests-toolbelt, strip-hints, tabulate, typer, typing-extensions, uritemplate\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "web_downloader = kfp.components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/contrib/web/Download/component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_packages = ['pandas==1.3.5', 'numpy==1.21.6', 'scikit-learn==1.0.2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_path: comp.InputPath(), train_output_csv: comp.OutputPath(), test_output_csv: comp.OutputPath()):\n",
    "    import pandas as pd\n",
    "    from zipfile import ZipFile\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    def check_null(df):\n",
    "        \"\"\" \n",
    "        Fucntion to check null values\n",
    "        \"\"\"\n",
    "        null_counts = df.isnull().sum()\n",
    "        total_rows = len(df)\n",
    "        null_ratios = null_counts / total_rows\n",
    "    \n",
    "        # Create a DataFrame to hold the result\n",
    "        result_df = pd.DataFrame({\n",
    "            'column_name': null_counts.index,\n",
    "            'null_count' : null_counts.values,\n",
    "            'null_ratio': null_ratios.values\n",
    "        })\n",
    "        return result_df\n",
    "        \n",
    "    # Extracting from zip file \n",
    "    with ZipFile(file_path, 'r') as zip:\n",
    "        zip.extractall()\n",
    "    \n",
    "    file_dir = 'Housing.csv'\n",
    "    df = pd.read_csv(file_dir)\n",
    "    check_null_df = check_null(df)\n",
    "    print('shape:', df.shape)\n",
    "    print(check_null_df)\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.3, random_state=0)\n",
    "    train_df.to_csv(train_output_csv, index=False)\n",
    "    test_df.to_csv(test_output_csv, index=False)   \n",
    "    print(\"\\n ---- train and test csv is saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_data(train_path: comp.InputPath(), cleaned_train_output_csv:comp. OutputPath()):\n",
    "    import pandas as pd\n",
    "\n",
    "    def get_feature_lists(df, target):\n",
    "        \"\"\"\n",
    "        Function to categorize DataFrame columns into categorical, numerical, and date columns\n",
    "        \"\"\"\n",
    "        categorical_feature = []\n",
    "        numerical_feature = []\n",
    "        date_columns = []\n",
    "        for col in df.drop(columns=target).columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                categorical_feature.append(col)\n",
    "            elif df[col].dtype == 'datetime64[ns]':\n",
    "                date_columns.append(col)\n",
    "            else:\n",
    "                numerical_feature.append(col)\n",
    "        return categorical_feature, numerical_feature, date_columns\n",
    "    \n",
    "    def convert_columns_to_binary(df, columns_to_convert):\n",
    "        \"\"\"\n",
    "        Function to convert specified columns in a DataFrame to binary format\n",
    "        \"\"\"\n",
    "        for col in columns_to_convert:\n",
    "            df[col] = df[col].map(lambda x: 1 if x == 'yes' else 0)\n",
    "        return df\n",
    "\n",
    "    def convert_furnish_column(df, type, categorical_column_list):\n",
    "        \"\"\"\n",
    "        Function to convert categorical\n",
    "        \"\"\"   \n",
    "        if type == 'train':\n",
    "            df = pd.get_dummies(df, drop_first=True, columns = categorical_column_list)\n",
    "        else: \n",
    "            df = pd.get_dummies(df,drop_first=False, columns = categorical_column_list)\n",
    "        return df\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    target = 'price'\n",
    "    categorical_feature, numerical_feature, date_columns = get_feature_lists(train_df, target)\n",
    "    print('categorical feature:', categorical_feature)\n",
    "    print('numerical_feature:', numerical_feature)\n",
    "    print('date column:', date_columns)\n",
    "\n",
    "    \n",
    "    columns_to_convert_bool = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
    "    train_df = convert_columns_to_binary(train_df, columns_to_convert_bool)\n",
    "\n",
    "    train_df = convert_furnish_column(train_df, 'train', ['furnishingstatus',])\n",
    "\n",
    "    train_df.fillna(0, inplace=True)\n",
    "    train_df.to_csv(cleaned_train_output_csv, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(cleaned_train_path: comp.InputPath(), model: comp.OutputPath()):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    train_df = pd.read_csv(cleaned_train_path)\n",
    "    target = 'price'\n",
    "    x_train = train_df.drop(columns=[target])\n",
    "    y_train = train_df[target]\n",
    "    \n",
    "    rf_regressor = RandomForestRegressor(n_estimators=25, random_state=42)\n",
    "    rf_regressor.fit(x_train, y_train)\n",
    "    \n",
    "    \n",
    "    # with open(f'workspace/model.pkl', 'wb') as f:\n",
    "    with open(model, 'wb') as f:\n",
    "        pickle.dump(rf_regressor, f)\n",
    "    \n",
    "    print(\"\\n rf regressor wass trained and saved to /data/model.pkl ----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict_on_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test_data(cleaned_train_df_path: comp.InputPath(), test_df_path: comp.InputPath(), model: comp.InputPath(), cleaned_test_result_csv: comp.OutputPath()):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "    def convert_columns_to_binary(df, columns_to_convert):\n",
    "        \"\"\"\n",
    "        Function to convert specified columns in a DataFrame to binary format\n",
    "        \"\"\"\n",
    "        for col in columns_to_convert:\n",
    "            df[col] = df[col].map(lambda x: 1 if x == 'yes' else 0)\n",
    "        return df\n",
    "\n",
    "    def convert_furnish_column(df, type, categorical_column_list):\n",
    "        \"\"\"\n",
    "        Function to convert categorical\n",
    "        \"\"\"   \n",
    "        if type == 'train':\n",
    "            df = pd.get_dummies(df, drop_first=True, columns = categorical_column_list)\n",
    "        else: \n",
    "            df = pd.get_dummies(df,drop_first=False, columns = categorical_column_list)\n",
    "        return df\n",
    "    \n",
    "    # with open(f'workspace/model.pkl','rb') as f:\n",
    "    with open(model,'rb') as f:\n",
    "         rf_regressor = pickle.load(f)\n",
    "\n",
    "    target = 'price'\n",
    "    x_train = pd.read_csv(cleaned_train_df_path).head().drop(columns=target)\n",
    "    test_df = pd.read_csv(test_df_path)\n",
    "    x_test = test_df.drop(columns=[target])\n",
    "    y_test = test_df[target]\n",
    "\n",
    "    columns_to_convert_bool = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
    "    x_test = convert_columns_to_binary(x_test, columns_to_convert_bool)\n",
    "    x_test = convert_furnish_column(x_test, 'test', ['furnishingstatus',])\n",
    "\n",
    "    #make sure train and test set have the same number of columns \n",
    "    _, x_test = x_train.align(x_test,join='left',axis=1)\n",
    "    x_test.fillna(0, inplace=True)\n",
    "\n",
    "    x_test['prediction'] = rf_regressor.predict(x_test)\n",
    "    x_test[target] = y_test\n",
    "\n",
    "    x_test.to_csv(cleaned_test_result_csv, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(test_result_path: comp.InputPath(), cleaned_train_df_path: comp.InputPath()):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    target = 'price'\n",
    "    test_df = pd.read_csv(test_result_path)[[target,'prediction']]\n",
    "    train_df = pd.read_csv(cleaned_train_df_path)\n",
    "    print('std of full data:', np.std(test_df[target].append(train_df[target])))\n",
    "    mse = mean_squared_error(test_df[target], test_df['prediction'])\n",
    "    print('rmse of test:', np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store model to gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model= kfp.components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/112de249a2c252f0a636bbfdf469d7ef2456f286/components/google-cloud/storage/upload_to_explicit_uri/component.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kubeflow pipeline creation work start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_packages = ['pandas==1.3.5', 'numpy==1.21.6', 'scikit-learn==1.0.2']\n",
    "\n",
    "prepare_data_op = kfp.components.create_component_from_func(\n",
    "    func=prepare_data,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=import_packages\n",
    ")\n",
    "preprocess_training_data_op = kfp.components.create_component_from_func(\n",
    "    func=preprocess_training_data,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=import_packages\n",
    ")\n",
    "train_model_op = kfp.components.create_component_from_func(\n",
    "    func=train_model,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=import_packages\n",
    ")\n",
    "predict_on_test_data_op = kfp.components.create_component_from_func(\n",
    "    func=predict_on_test_data,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=import_packages\n",
    ")\n",
    "get_metrics_op = kfp.components.create_component_from_func(\n",
    "    func=get_metrics,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=import_packages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='House Price Prediction Kubeflow Pipeline',\n",
    "   description='A pipeline that performs house price prediction'\n",
    ")\n",
    "\n",
    "# # Define parameters to be fed into pipeline\n",
    "def house_price_pipeline(url, gcs_model_path):\n",
    "\n",
    "    # # if output or input using component input/output path, code below might be necessary\n",
    "    # pv = dsl.VolumeOp(\n",
    "    # name=\"t-vol\",\n",
    "    # resource_name=\"t-vol\", \n",
    "    # size=\"1Gi\", \n",
    "    # modes=dsl.VOLUME_MODE_RWO)\n",
    "    # persistent_volume_path = '/workspace'\n",
    "    \n",
    "    web_downloader_task = web_downloader(url=url)\n",
    "\n",
    "    prepare_data_task = prepare_data_op(web_downloader_task.outputs['data'])\n",
    "\n",
    "    preprocess_training_data_task = preprocess_training_data_op(prepare_data_task.outputs['train_output_csv'])\n",
    "    \n",
    "    train_model_task = train_model_op(preprocess_training_data_task.outputs['cleaned_train_output_csv'])#.add_pvolumes({persistent_volume_path: pv.volume})\n",
    "\n",
    "    predict_on_test_data_task = predict_on_test_data_op(preprocess_training_data_task.outputs['cleaned_train_output_csv'], prepare_data_task.outputs['test_output_csv'],train_model_task.outputs['model']).after(train_model_task)\n",
    "                                                    # .add_pvolumes({persistent_volume_path: pv.volume}).after(train_model_task)\n",
    "                                                        \n",
    "    \n",
    "    get_metrics_task = get_metrics_op(predict_on_test_data_task.outputs['cleaned_test_result_csv'], preprocess_training_data_task.outputs['cleaned_train_output_csv'])\n",
    "\n",
    "    save_model_task = save_model(data=train_model_task.outputs['model'],gcs_path=gcs_model_path).apply(gcp.use_gcp_secret('user-gcp-sa')).after(get_metrics_task)\n",
    "                                                                                                #.add_pvolumes({persistent_volume_path: pv.volume}).after(get_metrics_task)\n",
    "    \n",
    "    web_downloader_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    prepare_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    preprocess_training_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    train_model_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    predict_on_test_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    get_metrics_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    save_model_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set gcs_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/add59ea5-9203-45ae-956c-b394e7141ad5\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/edf584db-ad2b-4fa3-9465-08328536766a\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now().date())\n",
    "\n",
    "\n",
    "pipeline_func = house_price_pipeline\n",
    "experiment_name = 'house_price_exp' +\"_\"+ str(datetime.datetime.now().date())\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "namespace = 'kubeflow'\n",
    "\n",
    "gcs_model_path = 'gs://<your_bucket>/house_price_rf.pkl'\n",
    "arguments = {'url': 'https://drive.google.com/uc?export=download&id=1QYduO-649A1ZhHUg46lwQ71dK1n_Au1Y',\n",
    "            'gcs_model_path' : gcs_model_path,\n",
    "            }\n",
    "\n",
    "\n",
    "client = kfp.Client() # change arguments accordingly\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
